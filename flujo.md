 ğŸ”„ Flujo Completo Paso a Paso

  FASE 1: PreparaciÃ³n del Entorno âš™ï¸

  1. GitHub Actions Trigger
     â”œâ”€ Detecta: git push a main
     â”œâ”€ Verifica: cambios en data-tickets-train/** o scripts/**
     â””â”€ Inicia: Runner Ubuntu-latest

  2. Setup Inicial
     â”œâ”€ Checkout cÃ³digo (actions/checkout@v3)
     â”œâ”€ Setup Python 3.9 (actions/setup-python@v4)
     â”œâ”€ Cache pip dependencies
     â””â”€ Install requirements.txt (189 paquetes)

  3. NLTK Resources Download
     â”œâ”€ punkt (tokenizador)
     â”œâ”€ stopwords (espaÃ±ol)
     â”œâ”€ wordnet (lematizaciÃ³n)
     â””â”€ omw-1.4 (Open Multilingual Wordnet)

  4. DVC Configuration
     â”œâ”€ Configure AWS credentials:
     â”‚  â”œâ”€ AWS_ACCESS_KEY_ID (secrets)
     â”‚  â”œâ”€ AWS_SECRET_ACCESS_KEY (secrets)
     â”‚  â””â”€ AWS_DEFAULT_REGION
     â”œâ”€ DVC remote: s3://tu-bucket/path
     â””â”€ DVC pull dataset_tickets.csv desde S3

  ---
  FASE 2: Carga y Preprocesamiento de Datos ğŸ“Š

  5. Load Dataset
     â”œâ”€ Lectura: data-tickets-train/dataset_tickets.csv
     â”œâ”€ ValidaciÃ³n: columnas requeridas ['texto', 'etiqueta']
     â”œâ”€ Shape: ~1,213 tickets Ã— 2 columnas
     â””â”€ DistribuciÃ³n de clases:
        â”œâ”€ TI: ~300 tickets
        â”œâ”€ RRHH: ~300 tickets
        â”œâ”€ Finanzas: ~300 tickets
        â””â”€ Operaciones: ~313 tickets

  6. NLP Preprocessing Pipeline (utils/preprocessing_data.py)

     Para CADA ticket:

     a) TokenizaciÃ³n
        â””â”€ NLTK word_tokenize() â†’ lista de tokens

     b) Lowercase
        â””â”€ "Mi Computadora NO Funciona" â†’ "mi computadora no funciona"

     c) Stopwords Removal
        â”œâ”€ NLTK stopwords espaÃ±ol base (183 palabras)
        â”œâ”€ Custom stopwords adicionales del config.yaml:
        â”‚  ["favor", "cordial", "saludo", "gracias", "atentamente", ...]
        â””â”€ Filtrado: [t for t in tokens if t not in stopwords]

     d) Cleaning
        â”œâ”€ Eliminar puntuaciÃ³n: string.punctuation
        â”œâ”€ Eliminar nÃºmeros standalone
        â”œâ”€ Eliminar tokens < 2 caracteres
        â””â”€ Strip whitespaces

     e) Stemming Snowball (espaÃ±ol)
        â”œâ”€ SnowballStemmer('spanish')
        â”œâ”€ "computadora" â†’ "comput"
        â”œâ”€ "problemas" â†’ "problem"
        â””â”€ Reduce dimensionalidad manteniendo semÃ¡ntica

     Ejemplo completo:
     Input:  "Por favor, mi computadora no funciona correctamente. Gracias"
     Output: "comput funcion correct"

  7. Feature Extraction: TF-IDF Vectorization

     â”œâ”€ TfidfVectorizer(
     â”‚     max_features=5000,        # Top 5000 tÃ©rminos mÃ¡s importantes
     â”‚     ngram_range=(1, 2),       # Unigrams + Bigrams
     â”‚     min_df=2,                 # MÃ­nimo 2 documentos
     â”‚     max_df=0.8,               # MÃ¡ximo 80% de documentos
     â”‚     sublinear_tf=True         # Escala logarÃ­tmica de TF
     â”‚  )
     â”‚
     â”œâ”€ Fit en datos de entrenamiento
     â”œâ”€ Transform: texto â†’ vector [5000 dimensiones]
     â”‚
     â””â”€ Resultado: Matriz sparse (1213, 5000)
        â€¢ Cada fila = 1 ticket
        â€¢ Cada columna = 1 tÃ©rmino
        â€¢ Valores = TF-IDF score [0, 1]

  ---
  FASE 3: Train/Test Split ğŸ”€

  8. Stratified Split

     â”œâ”€ train_test_split(
     â”‚     X=tfidf_matrix,
     â”‚     y=labels,
     â”‚     test_size=0.2,           # 20% test
     â”‚     stratify=y,              # Mantiene proporciÃ³n de clases
     â”‚     random_state=42          # Reproducibilidad
     â”‚  )
     â”‚
     â”œâ”€ X_train: 970 samples Ã— 5000 features
     â”œâ”€ X_test:  243 samples Ã— 5000 features
     â”œâ”€ y_train: 970 labels
     â””â”€ y_test:  243 labels

  9. Reproducibilidad Seeds
     
     â”œâ”€ Python: random.seed(42)
     â”œâ”€ NumPy: np.random.seed(42)
     â”œâ”€ Env: PYTHONHASHSEED=42
     â””â”€ Sklearn: random_state=42 en todos los modelos

  ---
  FASE 4: Entrenamiento de 7 Modelos ğŸ¤–

  10. Training Loop con Optuna Optimization

  Para CADA uno de los 7 modelos:

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ MODELO 1: Logistic Regression                       â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Optuna Hyperparameter Search:                       â”‚
  â”‚   â€¢ Trials: 10 (en CI/CD) o 20 (local)             â”‚
  â”‚   â€¢ Sampler: TPE (Tree-structured Parzen Estimator) â”‚
  â”‚   â€¢ Objective: Maximizar F1-score (macro avg)       â”‚
  â”‚                                                      â”‚
  â”‚ Hyperparameters a optimizar:                        â”‚
  â”‚   â€¢ C: [0.01, 100] (log scale)                     â”‚
  â”‚   â€¢ penalty: ['l1', 'l2']                           â”‚
  â”‚   â€¢ solver: ['liblinear', 'saga']                   â”‚
  â”‚   â€¢ max_iter: [100, 500]                            â”‚
  â”‚                                                      â”‚
  â”‚ Cross Validation:                                    â”‚
  â”‚   â€¢ StratifiedKFold (2 folds en CI, 3 en local)    â”‚
  â”‚   â€¢ MÃ©trica: f1_score(average='macro')              â”‚
  â”‚                                                      â”‚
  â”‚ Mejor configuraciÃ³n encontrada:                     â”‚
  â”‚   â€¢ C: 10.5                                         â”‚
  â”‚   â€¢ penalty: 'l2'                                   â”‚
  â”‚   â€¢ solver: 'liblinear'                             â”‚
  â”‚                                                      â”‚
  â”‚ Training final con mejores hiperparÃ¡metros          â”‚
  â”‚   â€¢ Fit en X_train completo                         â”‚
  â”‚   â€¢ Predict en X_test                               â”‚
  â”‚                                                      â”‚
  â”‚ EvaluaciÃ³n:                                         â”‚
  â”‚   â€¢ F1-score: 0.9712                                â”‚
  â”‚   â€¢ Accuracy: 0.9720                                â”‚
  â”‚   â€¢ Precision: 0.9715                               â”‚
  â”‚   â€¢ Recall: 0.9711                                  â”‚
  â”‚                                                      â”‚
  â”‚ MLflow Logging:                                      â”‚
  â”‚   â”œâ”€ log_params(C, penalty, solver)                â”‚
  â”‚   â”œâ”€ log_metrics(f1, accuracy, precision, recall)  â”‚
  â”‚   â””â”€ log_model(sklearn_model)                      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ MODELO 2: Random Forest                             â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Hyperparameters:                                     â”‚
  â”‚   â€¢ n_estimators: [100, 500]                        â”‚
  â”‚   â€¢ max_depth: [10, 50, None]                       â”‚
  â”‚   â€¢ min_samples_split: [2, 10]                      â”‚
  â”‚   â€¢ min_samples_leaf: [1, 4]                        â”‚
  â”‚   â€¢ max_features: ['sqrt', 'log2']                  â”‚
  â”‚                                                      â”‚
  â”‚ Mejor config:                                        â”‚
  â”‚   â€¢ n_estimators: 300                               â”‚
  â”‚   â€¢ max_depth: None                                 â”‚
  â”‚   â€¢ min_samples_split: 2                            â”‚
  â”‚                                                      â”‚
  â”‚ EvaluaciÃ³n:                                         â”‚
  â”‚   â€¢ F1-score: 0.9132                                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ MODELO 3: XGBoost                                   â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Hyperparameters:                                     â”‚
  â”‚   â€¢ n_estimators: [100, 500]                        â”‚
  â”‚   â€¢ max_depth: [3, 10]                              â”‚
  â”‚   â€¢ learning_rate: [0.01, 0.3]                      â”‚
  â”‚   â€¢ subsample: [0.6, 1.0]                           â”‚
  â”‚   â€¢ colsample_bytree: [0.6, 1.0]                    â”‚
  â”‚   â€¢ gamma: [0, 5]                                   â”‚
  â”‚                                                      â”‚
  â”‚ EvaluaciÃ³n:                                         â”‚
  â”‚   â€¢ F1-score: 0.9627                                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ MODELO 4: SVM (Support Vector Machine)             â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Hyperparameters:                                     â”‚
  â”‚   â€¢ C: [0.1, 100] (log scale)                       â”‚
  â”‚   â€¢ kernel: ['linear', 'rbf']                       â”‚
  â”‚   â€¢ gamma: ['scale', 'auto']                        â”‚
  â”‚                                                      â”‚
  â”‚ EvaluaciÃ³n:                                         â”‚
  â”‚   â€¢ F1-score: 0.9177                                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ MODELO 5: LightGBM                                  â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Hyperparameters:                                     â”‚
  â”‚   â€¢ n_estimators: [100, 500]                        â”‚
  â”‚   â€¢ max_depth: [3, 10]                              â”‚
  â”‚   â€¢ learning_rate: [0.01, 0.3]                      â”‚
  â”‚   â€¢ num_leaves: [20, 100]                           â”‚
  â”‚   â€¢ min_child_samples: [10, 50]                     â”‚
  â”‚                                                      â”‚
  â”‚ EvaluaciÃ³n:                                         â”‚
  â”‚   â€¢ F1-score: 0.9670                                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ MODELO 6: Gradient Boosting â­ WINNER               â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Hyperparameters:                                     â”‚
  â”‚   â€¢ n_estimators: [100, 500]                        â”‚
  â”‚   â€¢ max_depth: [3, 10]                              â”‚
  â”‚   â€¢ learning_rate: [0.01, 0.3]                      â”‚
  â”‚   â€¢ subsample: [0.6, 1.0]                           â”‚
  â”‚   â€¢ min_samples_split: [2, 10]                      â”‚
  â”‚                                                      â”‚
  â”‚ Mejor config encontrada:                            â”‚
  â”‚   â€¢ n_estimators: 400                               â”‚
  â”‚   â€¢ max_depth: 7                                    â”‚
  â”‚   â€¢ learning_rate: 0.1                              â”‚
  â”‚   â€¢ subsample: 0.9                                  â”‚
  â”‚                                                      â”‚
  â”‚ EvaluaciÃ³n: ğŸ†                                      â”‚
  â”‚   â€¢ F1-score: 0.9835 â† MEJOR                        â”‚
  â”‚   â€¢ Accuracy: 0.9835                                â”‚
  â”‚   â€¢ Precision: 0.9838                               â”‚
  â”‚   â€¢ Recall: 0.9833                                  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ MODELO 7: Extra Trees                              â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Hyperparameters:                                     â”‚
  â”‚   â€¢ n_estimators: [100, 500]                        â”‚
  â”‚   â€¢ max_depth: [10, 50, None]                       â”‚
  â”‚   â€¢ min_samples_split: [2, 10]                      â”‚
  â”‚                                                      â”‚
  â”‚ EvaluaciÃ³n:                                         â”‚
  â”‚   â€¢ F1-score: 0.9134                                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  11. ComparaciÃ³n y SelecciÃ³n del Mejor Modelo

     Resultados Finales:
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Modelo              â”‚ F1-Score â”‚ Accuracy â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚ Gradient Boosting   â”‚ 0.9835   â”‚ 0.9835   â”‚ â­ SELECCIONADO
     â”‚ Logistic Regression â”‚ 0.9712   â”‚ 0.9720   â”‚
     â”‚ LightGBM           â”‚ 0.9670   â”‚ 0.9670   â”‚
     â”‚ XGBoost            â”‚ 0.9627   â”‚ 0.9630   â”‚
     â”‚ SVM                â”‚ 0.9177   â”‚ 0.9180   â”‚
     â”‚ Extra Trees        â”‚ 0.9134   â”‚ 0.9140   â”‚
     â”‚ Random Forest      â”‚ 0.9132   â”‚ 0.9135   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

     Criterio de selecciÃ³n:
     â”œâ”€ Maximizar F1-score (macro average)
     â”œâ”€ En caso de empate: preferir menor complejidad
     â””â”€ Winner: Gradient Boosting (F1: 0.9835)

  ---
  FASE 5: Guardado y Versionamiento ğŸ’¾

  12. SerializaciÃ³n del Mejor Modelo

     â”œâ”€ Crear objeto de pipeline completo:
     â”‚  {
     â”‚    'vectorizer': TfidfVectorizer (fitted),
     â”‚    'model': GradientBoostingClassifier (trained),
     â”‚    'label_encoder': LabelEncoder (fitted),
     â”‚    'preprocessing_config': {...}
     â”‚  }
     â”‚
     â”œâ”€ Guardar con pickle:
     â”‚  â””â”€ models/best_model.pkl (tamaÃ±o: ~50 MB)
     â”‚
     â””â”€ Crear backup con timestamp:
        â””â”€ models/backups/best_model_20251213_154530.pkl

  13. Guardar Metadata JSON

     models/best_model_metadata.json:
     {
       "model_name": "Gradient_Boosting",
       "f1_score": 0.9835,
       "accuracy": 0.9835,
       "precision": 0.9838,
       "recall": 0.9833,
       "timestamp": "2025-12-13T15:45:30",
       "environment": "CI/CD",
       "training_samples": 970,
       "test_samples": 243,
       "features_count": 5000,
       "hyperparameters": {
         "n_estimators": 400,
         "max_depth": 7,
         "learning_rate": 0.1,
         "subsample": 0.9
       },
       "training_config": {
         "random_seed": 42,
         "cv_folds": 2,
         "optuna_trials": 10,
         "max_features": 5000
       },
       "all_results": {
         "Logistic_Regression": {"f1": 0.9712, ...},
         "Random_Forest": {"f1": 0.9132, ...},
         "XGBoost": {"f1": 0.9627, ...},
         "SVM": {"f1": 0.9177, ...},
         "LightGBM": {"f1": 0.9670, ...},
         "Gradient_Boosting": {"f1": 0.9835, ...},
         "Extra_Trees": {"f1": 0.9134, ...}
       },
       "confusion_matrix": [[...], [...], [...], [...]],
       "classification_report": {...}
     }

  14. DVC Versionamiento

     â”œâ”€ dvc add models/best_model.pkl
     â”‚  â”œâ”€ Genera: models/best_model.pkl.dvc (puntero)
     â”‚  â”œâ”€ Calcula: MD5 hash del modelo
     â”‚  â””â”€ Mueve archivo a: .dvc/cache/
     â”‚
     â”œâ”€ dvc push
     â”‚  â”œâ”€ Sube modelo a S3
     â”‚  â”œâ”€ Path: s3://bucket/models/md5hash
     â”‚  â””â”€ Actualiza remote
     â”‚
     â””â”€ git add models/best_model.pkl.dvc
        â””â”€ Commitea solo el puntero (lightweight)

  15. MLflow Registry

     â”œâ”€ mlflow.sklearn.log_model(
     â”‚     sk_model=best_model,
     â”‚     artifact_path="gradient_boosting_model",
     â”‚     registered_model_name="TicketClassifier"
     â”‚  )
     â”‚
     â”œâ”€ Guarda en: mlruns/
     â”‚  â”œâ”€ Experiment ID
     â”‚  â”œâ”€ Run ID
     â”‚  â”œâ”€ Artifacts/
     â”‚  â”œâ”€ Metrics/
     â”‚  â””â”€ Params/
     â”‚
     â””â”€ Versionado automÃ¡tico: v1, v2, v3...

  ---
  FASE 6: Artifacts Upload (GitHub Actions) ğŸ“¤

  16. Upload Artifacts to GitHub

     â”œâ”€ actions/upload-artifact@v3
     â”‚
     â”œâ”€ Artifact 1: trained-model
     â”‚  â”œâ”€ models/best_model.pkl
     â”‚  â””â”€ models/best_model_metadata.json
     â”‚
     â”œâ”€ Artifact 2: mlflow-runs
     â”‚  â””â”€ mlruns/ (completo)
     â”‚
     â””â”€ RetenciÃ³n: 90 dÃ­as

  ---
  FASE 7: Hot Reload de API ğŸ”„

  17. Reload Model en API (sin downtime)

     â”œâ”€ Endpoint: POST /admin/reload-model
     â”‚  â””â”€ Headers: X-API-Key: $ADMIN_API_KEY
     â”‚
     â”œâ”€ API descarga nuevo modelo:
     â”‚  â”œâ”€ dvc pull models/best_model.pkl
     â”‚  â””â”€ Load desde S3
     â”‚
     â”œâ”€ Recarga en memoria:
     â”‚  â”œâ”€ global model_pipeline
     â”‚  â”œâ”€ model_pipeline = pickle.load(...)
     â”‚  â””â”€ Log: "Model reloaded successfully"
     â”‚
     â””â”€ Zero downtime:
        â€¢ No reinicia uvicorn
        â€¢ Requests en proceso continÃºan con modelo anterior
        â€¢ Nuevos requests usan modelo nuevo

  ---
  FASE 8: Summary Report ğŸ“Š

  18. GitHub Actions Summary

     Genera reporte markdown automÃ¡tico:

     ## ğŸ‹ï¸ Training Pipeline - Completed Successfully

     ### Best Model Selected
     - **Algorithm**: Gradient Boosting
     - **F1-Score**: 0.9835
     - **Accuracy**: 0.9835
     - **Training Time**: 58.3 minutes

     ### All Models Performance
     | Model | F1-Score | Accuracy | Training Time |
     |-------|----------|----------|---------------|
     | Gradient Boosting | 0.9835 | 0.9835 | 12.5 min |
     | Logistic Regression | 0.9712 | 0.9720 | 3.2 min |
     | LightGBM | 0.9670 | 0.9670 | 8.7 min |
     | XGBoost | 0.9627 | 0.9630 | 10.1 min |
     | SVM | 0.9177 | 0.9180 | 15.6 min |
     | Extra Trees | 0.9134 | 0.9140 | 6.8 min |
     | Random Forest | 0.9132 | 0.9135 | 7.4 min |

     ### Hyperparameters
     ```json
     {
       "n_estimators": 400,
       "max_depth": 7,
       "learning_rate": 0.1,
       "subsample": 0.9
     }

  Confusion Matrix

             TI  RRHH  Finanzas  Ops
  TI        59    0       1       0
  RRHH       0   60       0       1
  Finanzas   1    0      59       0
  Ops        0    1       0      61

  Next Steps

     âœ… Model pushed to S3
     âœ… API reloaded
     âœ… Ready for production
